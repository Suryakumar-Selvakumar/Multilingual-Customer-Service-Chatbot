{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574fd185",
   "metadata": {},
   "source": [
    "# Task 3 - Training M-bart\n",
    "\n",
    "- *Setup Notebook on Google Colab*\n",
    "- *Load the MBart Model and Tokenizer from Google Drive*\n",
    "- *Tokenize the Multilingual Dataset*\n",
    "- *Split the Dataset into 80% Train set and 20% Test set*\n",
    "- *Setup Training Arguements and Trainer for MBart*\n",
    "- *Train MBart and Save the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54ddb653-6a9e-46f9-9735-2d4349a9b499",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1945,
     "status": "ok",
     "timestamp": 1728853965639,
     "user": {
      "displayName": "Suryakumar S",
      "userId": "07994311158553412951"
     },
     "user_tz": 360
    },
    "id": "54ddb653-6a9e-46f9-9735-2d4349a9b499",
    "outputId": "f6713353-d2d4-4373-8d66-8fc9a2f186dd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# Ensuring GPU is detected\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "OQDTdiSItTp-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 713,
     "status": "ok",
     "timestamp": 1728853966350,
     "user": {
      "displayName": "Suryakumar S",
      "userId": "07994311158553412951"
     },
     "user_tz": 360
    },
    "id": "OQDTdiSItTp-",
    "outputId": "f6fc4315-81e7-4b7b-9082-63601ba7a512"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "twCMso8FRhmI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5184,
     "status": "ok",
     "timestamp": 1728853974029,
     "user": {
      "displayName": "Suryakumar S",
      "userId": "07994311158553412951"
     },
     "user_tz": 360
    },
    "id": "twCMso8FRhmI",
    "outputId": "95a653de-3ded-421b-f084-f3439f7adaa7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = \"drive/MyDrive/Multi-lingual Customer Service Chatbot - Colab/models/mbart-large-50\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_path, padding_side='right')\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0d4d7f",
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1728853974420,
     "user": {
      "displayName": "Suryakumar S",
      "userId": "07994311158553412951"
     },
     "user_tz": 360
    },
    "id": "ef0d4d7f"
   },
   "outputs": [],
   "source": [
    "# Define tokenization function without static padding (dynamic padding will be done later)\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['instruction'], truncation=True, padding=False, max_length=20)  # 95th Percentile of instruction_length\n",
    "    targets = tokenizer(examples['response'], truncation=True, padding=False, max_length=440)   # 99th Percentile of response_length\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Load and tokenize dataset\n",
    "dataset = load_dataset('csv', data_files='drive/MyDrive/Multi-lingual Customer Service Chatbot - Colab/data/Multilingual_Customer_Support_Training_Dataset.csv')\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a74b8286",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5495,
     "status": "ok",
     "timestamp": 1728853979913,
     "user": {
      "displayName": "Suryakumar S",
      "userId": "07994311158553412951"
     },
     "user_tz": 360
    },
    "id": "a74b8286",
    "outputId": "5b8b7b15-357d-4888-c27c-493d35f9306d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and Trainer initialized for Model Training\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForSeq2Seq\n",
    "from datasets import concatenate_datasets\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_dir = \"drive/MyDrive/Multi-lingual Customer Service Chatbot - Colab/models/final_mbart_model_new\"\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Corrected number of samples per language (since each language has 26,872 rows)\n",
    "    sample_size_per_language = 26872\n",
    "\n",
    "    # Filter dataset by language and take samples for each language\n",
    "    en_samples = tokenized_datasets[\"train\"].filter(lambda example: example['language'] == 'en').shuffle(seed=42).select(range(sample_size_per_language))\n",
    "    fr_samples = tokenized_datasets[\"train\"].filter(lambda example: example['language'] == 'fr').shuffle(seed=42).select(range(sample_size_per_language))\n",
    "    es_samples = tokenized_datasets[\"train\"].filter(lambda example: example['language'] == 'es').shuffle(seed=42).select(range(sample_size_per_language))\n",
    "\n",
    "    # Combine samples from all languages to form the full train dataset\n",
    "    full_dataset = concatenate_datasets([en_samples, fr_samples, es_samples])\n",
    "\n",
    "    # Split the dataset into training (80%) and evaluation (20%)\n",
    "    train_size = int(0.80 * len(full_dataset))\n",
    "    eval_size = len(full_dataset) - train_size\n",
    "\n",
    "    train_dataset = full_dataset.select(range(train_size))\n",
    "    eval_dataset = full_dataset.select(range(train_size, len(full_dataset)))\n",
    "\n",
    "    # Create a data collator for dynamic padding\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)  # Padding=True enables dynamic padding\n",
    "\n",
    "    # Modified training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"drive/MyDrive/Multi-lingual Customer Service Chatbot - Colab/models\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,  # Slightly higher learning rate to start\n",
    "        per_device_train_batch_size=12,  # Batch size for efficient memory usage\n",
    "        per_device_eval_batch_size=12,   # Same batch size for evaluation\n",
    "        num_train_epochs=7,  # Increased epochs for better generalization\n",
    "        save_total_limit=3,  # Save checkpoints to prevent excessive memory use\n",
    "        fp16=True,  # Mixed precision training for performance improvements\n",
    "        weight_decay=0.01,  # Helps with regularization and avoiding overfitting\n",
    "        logging_steps=200,\n",
    "        load_best_model_at_end=True,\n",
    "        gradient_accumulation_steps=5,  # Accumulating gradients to reduce memory footprint\n",
    "        warmup_steps=500,  # Reduced warmup steps for faster convergence\n",
    "        report_to=\"none\",  # Reporting disabled to avoid logging overhead\n",
    "        gradient_checkpointing=True,  # Save memory during gradient computation\n",
    "        lr_scheduler_type=\"cosine\",  # Cosine learning rate scheduler for smoother training\n",
    "        eval_accumulation_steps=10,  # Accumulate evaluation steps to avoid memory issues\n",
    "    )\n",
    "\n",
    "    # Trainer setup with dynamic padding and BLEU score computation\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,  # Enable dynamic padding\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Early stopping if validation loss doesn't improve\n",
    "    )\n",
    "    print(f\"Dataset and Trainer initialized for Model Training\")\n",
    "else:\n",
    "    print(f\"Model and tokenizer already exist in {model_dir}. Skipping initialization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9VJiTMznWdbM",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1728853979913,
     "user": {
      "displayName": "Suryakumar S",
      "userId": "07994311158553412951"
     },
     "user_tz": 360
    },
    "id": "9VJiTMznWdbM"
   },
   "outputs": [],
   "source": [
    "!PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "639fe732",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10099249,
     "status": "ok",
     "timestamp": 1728864079160,
     "user": {
      "displayName": "Suryakumar S",
      "userId": "07994311158553412951"
     },
     "user_tz": 360
    },
    "id": "639fe732",
    "outputId": "c604ebd1-0e20-4b7a-f796-3a132f6836b1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7525' max='7525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7525/7525 2:48:04, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.280400</td>\n",
       "      <td>1.306809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.738200</td>\n",
       "      <td>0.787249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.654700</td>\n",
       "      <td>0.748505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.590900</td>\n",
       "      <td>0.693028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.676866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.539300</td>\n",
       "      <td>0.670430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>0.670399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to drive/MyDrive/Multi-lingual Customer Service Chatbot - Colab/models/final_mbart_model_new\n"
     ]
    }
   ],
   "source": [
    "# Check if the model already exists\n",
    "if not os.path.exists(model_dir):\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the final model and tokenizer if it doesn't exist\n",
    "    trainer.save_model(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    print(f\"Model and tokenizer saved to {model_dir}\")\n",
    "else:\n",
    "    print(f\"Model and tokenizer already exist in {model_dir}. Skipping training.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chatbot-env",
   "language": "python",
   "name": "chatbot-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
